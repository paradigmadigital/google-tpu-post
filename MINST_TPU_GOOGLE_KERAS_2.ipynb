{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MINST_TPU_GOOGLE_KERAS_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQhc33E1UQ3e",
        "colab_type": "code",
        "outputId": "fac01c23-4eb2-49b7-d29b-c886edc7cf06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDDAJAIuQ9BY",
        "colab_type": "code",
        "outputId": "7b790ba5-520e-4744-ff38-97e26eda2a74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten,Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import backend as K\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_I53MUSMhZ1",
        "colab_type": "text"
      },
      "source": [
        "En caso de querer utiliar GPU o TPU necesitamos autenticarnos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZOmPtkcRC8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IS_COLAB_BACKEND = 'COLAB_GPU' in os.environ  # this is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
        "if IS_COLAB_BACKEND:\n",
        "  from google.colab import auth\n",
        "  # Authenticates the Colab machine and also the TPU using your\n",
        "  # credentials so that they can access your private GCS buckets.\n",
        "  auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GcuQXlnLN8u",
        "colab_type": "text"
      },
      "source": [
        "Generamos la estrategia a partir del hardware detectado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr9cGt0qRF-t",
        "colab_type": "code",
        "outputId": "f987251c-799d-4e31-d44d-b510afab2871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Detect hardware\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError:\n",
        "  tpu = None\n",
        "  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "    \n",
        "# Select appropriate distribution strategy\n",
        "if tpu:\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \n",
        "elif len(gpus) > 1:\n",
        "  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
        "elif len(gpus) == 1:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on single GPU ', gpus[0].name)\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on CPU')\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on single GPU  /device:GPU:0\n",
            "Number of accelerators:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmAJ2EgkRTxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64 * strategy.num_replicas_in_sync # Gobal batch size.\n",
        "# The global batch size will be automatically sharded across all\n",
        "# replicas by the tf.data.Dataset API. A single TPU has 8 cores.\n",
        "# The best practice is to scale the batch size by the number of\n",
        "# replicas (cores). The learning rate should be increased as well.\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "LEARNING_RATE_EXP_DECAY = 0.6 if strategy.num_replicas_in_sync == 1 else 0.7\n",
        "# Learning rate computed later as LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch\n",
        "# 0.7 decay instead of 0.6 means a slower decay, i.e. a faster learnign rate.\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 12\n",
        "\n",
        "SIZE_MULTIPLIER = 1\n",
        "IMG_ROWS, IMG_COLS = 28 * SIZE_MULTIPLIER, 28 * SIZE_MULTIPLIER\n",
        "\n",
        "training_images_file   = 'gs://mnist-public/train-images-idx3-ubyte'\n",
        "training_labels_file   = 'gs://mnist-public/train-labels-idx1-ubyte'\n",
        "validation_images_file = 'gs://mnist-public/t10k-images-idx3-ubyte'\n",
        "validation_labels_file = 'gs://mnist-public/t10k-labels-idx1-ubyte'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXSBK607Rbb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def read_label(tf_bytestring):\n",
        "    label = tf.io.decode_raw(tf_bytestring, tf.uint8)\n",
        "    label = tf.reshape(label, [])\n",
        "    label = tf.one_hot(label, NUM_CLASSES)\n",
        "    return label\n",
        "  \n",
        "def read_image(tf_bytestring):\n",
        "    image = tf.io.decode_raw(tf_bytestring, tf.uint8)\n",
        "    image = tf.cast(image, tf.float32)/255.0\n",
        "    image = tf.reshape(image, [IMG_ROWS*IMG_COLS])\n",
        "    return image\n",
        "  \n",
        "def load_dataset(image_file, label_file):\n",
        "    imagedataset = tf.data.FixedLengthRecordDataset(image_file, IMG_ROWS*IMG_COLS, header_bytes=16)\n",
        "    imagedataset = imagedataset.map(read_image, num_parallel_calls=16)\n",
        "    labelsdataset = tf.data.FixedLengthRecordDataset(label_file, 1, header_bytes=8)\n",
        "    labelsdataset = labelsdataset.map(read_label, num_parallel_calls=16)\n",
        "    dataset = tf.data.Dataset.zip((imagedataset, labelsdataset))\n",
        "    return dataset \n",
        "  \n",
        "def get_training_dataset(image_file, label_file, batch_size):\n",
        "    dataset = load_dataset(image_file, label_file)\n",
        "    dataset = dataset.cache()  # this small dataset can be entirely cached in RAM\n",
        "    dataset = dataset.shuffle(5000, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True) # drop_remainder is important on TPU, batch size must be fixed\n",
        "    dataset = dataset.prefetch(-1)  # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
        "    return dataset\n",
        "  \n",
        "def get_validation_dataset(image_file, label_file):\n",
        "    dataset = load_dataset(image_file, label_file)\n",
        "    dataset = dataset.cache() # this small dataset can be entirely cached in RAM\n",
        "    dataset = dataset.batch(10000, drop_remainder=True) # 10000 items in eval dataset, all in one batch\n",
        "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
        "    return dataset\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset = get_training_dataset(training_images_file, training_labels_file, BATCH_SIZE)\n",
        "validation_dataset = get_validation_dataset(validation_images_file, validation_labels_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pa7nVjefWb5",
        "colab_type": "text"
      },
      "source": [
        "Modificando la forma de carga del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nZWIelJRi3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This model trains to 99.4% accuracy in 10 epochs (with a batch size of 64)  \n",
        "\n",
        "def make_model():\n",
        "    model = tf.keras.Sequential(\n",
        "      [\n",
        "        tf.keras.layers.Reshape(input_shape=(IMG_ROWS*IMG_COLS,), target_shape=(IMG_ROWS, IMG_COLS, 1), name=\"image\"),\n",
        "\n",
        "        tf.keras.layers.Conv2D(filters=12, kernel_size=3, padding='same', use_bias=False), # no bias necessary before batch norm\n",
        "        tf.keras.layers.BatchNormalization(scale=False, center=True), # no batch norm scaling necessary before \"relu\"\n",
        "        tf.keras.layers.Activation('relu'), # activation after batch norm\n",
        "\n",
        "        tf.keras.layers.Conv2D(filters=24, kernel_size=6, padding='same', use_bias=False, strides=2),\n",
        "        tf.keras.layers.BatchNormalization(scale=False, center=True),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "\n",
        "        tf.keras.layers.Conv2D(filters=64, kernel_size=6, padding='same', use_bias=False, strides=2),\n",
        "        tf.keras.layers.BatchNormalization(scale=False, center=True),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(200, use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(scale=False, center=True),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "        tf.keras.layers.Dropout(0.4), # Dropout on dense layer only\n",
        "\n",
        "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "      ])\n",
        "\n",
        "    model.compile(optimizer='adam', # learning rate will be set by LearningRateScheduler\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "    \n",
        "with strategy.scope():\n",
        "    model = make_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbz5pbeyRl5S",
        "colab_type": "code",
        "outputId": "59430047-a00e-4152-8788-9c3a647a7103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# print model layers\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image (Reshape)              (None, 280, 280, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 280, 280, 12)      108       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 280, 280, 12)      36        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 280, 280, 12)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 140, 140, 24)      10368     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 140, 140, 24)      72        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 140, 140, 24)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 70, 70, 64)        55296     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 70, 70, 64)        192       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 70, 70, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 313600)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               62720000  \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 200)               600       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2010      \n",
            "=================================================================\n",
            "Total params: 62,788,682\n",
            "Trainable params: 62,788,082\n",
            "Non-trainable params: 600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAhfIuT0RoDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up learning rate decay\n",
        "lr_decay = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch,\n",
        "    verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_rDbQFy3t70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "class TimeHistory(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)\n",
        "\n",
        "time_callback = TimeHistory()\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDl0UQePRrcb",
        "colab_type": "code",
        "outputId": "d2f46aa1-a54e-4b9a-dbd7-d269b816e7c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "steps_per_epoch = 60000//BATCH_SIZE  # 60,000 items in this dataset\n",
        "print(\"Steps per epoch: \", steps_per_epoch)\n",
        "  \n",
        "# Little wrinkle: in the present version of Tensorfow (1.14), switching a TPU\n",
        "# between training and evaluation is slow (approx. 10 sec). For small models,\n",
        "# it is recommeneded to run a single eval at the end.\n",
        "\n",
        "history = model.fit(training_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                    callbacks=[lr_decay, time_callback], verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps per epoch:  937\n",
            "Train for 937 steps\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/12\n",
            "937/937 [==============================] - 93s 99ms/step - loss: 0.0534 - accuracy: 0.9881\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.006.\n",
            "Epoch 2/12\n",
            "937/937 [==============================] - 84s 90ms/step - loss: 1.2447e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0036.\n",
            "Epoch 3/12\n",
            "937/937 [==============================] - 84s 90ms/step - loss: 6.2369e-05 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0021599999999999996.\n",
            "Epoch 4/12\n",
            "937/937 [==============================] - 84s 90ms/step - loss: 4.1817e-05 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001296.\n",
            "Epoch 5/12\n",
            "937/937 [==============================] - 84s 90ms/step - loss: 2.9515e-05 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0007775999999999998.\n",
            "Epoch 6/12\n",
            "937/937 [==============================] - 84s 90ms/step - loss: 2.4266e-05 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0004665599999999999.\n",
            "Epoch 7/12\n",
            "937/937 [==============================] - 84s 90ms/step - loss: 2.1963e-05 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.00027993599999999994.\n",
            "Epoch 8/12\n",
            "937/937 [==============================] - 84s 90ms/step - loss: 1.8936e-05 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00016796159999999993.\n",
            "Epoch 9/12\n",
            "937/937 [==============================] - 85s 90ms/step - loss: 1.5133e-05 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00010077695999999997.\n",
            "Epoch 10/12\n",
            "937/937 [==============================] - 85s 90ms/step - loss: 1.2782e-05 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 6.0466175999999974e-05.\n",
            "Epoch 11/12\n",
            "937/937 [==============================] - 84s 90ms/step - loss: 1.2192e-05 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 3.627970559999999e-05.\n",
            "Epoch 12/12\n",
            "937/937 [==============================] - 85s 91ms/step - loss: 1.2758e-05 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcS__AnS4ejP",
        "colab_type": "text"
      },
      "source": [
        "Tiempos en cada epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMVx4PV34coD",
        "colab_type": "code",
        "outputId": "ba20433d-ef82-40b6-e97d-4cd7a99de27f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(time_callback.times)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[92.9989104270935, 84.10103988647461, 84.2870876789093, 84.3103129863739, 84.22848796844482, 84.24401640892029, 84.27055382728577, 84.0003411769867, 84.6183831691742, 84.68401598930359, 84.35533022880554, 84.86749625205994]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}